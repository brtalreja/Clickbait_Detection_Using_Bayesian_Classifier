{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup, Install, and import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from csv import QUOTE_NONE\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANT NOTE: THROUGHOUT THE PROGRAM, WE WILL CONSIDER CLICKBAIT AS '1' AND NOT-CLICKBAIT as '0'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "1) Read the clickbait data and the not clickbait data.\n",
    "2) Merging the two datasets.\n",
    "3) Shuffling the data after merging.\n",
    "4) Splitting the data into train-validation-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Man repairs fence to contain dog, hilarity ens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Long-Term Marijuana Use Has One Crazy Side Eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The water from his ear trickles into the bucke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You'll Never Guess What Nick Jonas Does in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How Cruise Liners Fill All Their Unsold Cruise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>Mark your calendars: Jon Stewart announces the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>OITNB's Taylor Schilling and Carrie Brownstein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>Researchers have discovered the average penis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>Why it may be smart to wait to put on sunscree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>What state has highest rate of rape in the cou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>814 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text\n",
       "0    Man repairs fence to contain dog, hilarity ens...\n",
       "1    Long-Term Marijuana Use Has One Crazy Side Eff...\n",
       "2    The water from his ear trickles into the bucke...\n",
       "3    You'll Never Guess What Nick Jonas Does in the...\n",
       "4    How Cruise Liners Fill All Their Unsold Cruise...\n",
       "..                                                 ...\n",
       "809  Mark your calendars: Jon Stewart announces the...\n",
       "810  OITNB's Taylor Schilling and Carrie Brownstein...\n",
       "811  Researchers have discovered the average penis ...\n",
       "812  Why it may be smart to wait to put on sunscree...\n",
       "813  What state has highest rate of rape in the cou...\n",
       "\n",
       "[814 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading clickbait dataset.\n",
    "string = []    \n",
    "with open(\"clickbait\", \"r\") as f:\n",
    "    all_lines = f.read()\n",
    "    for line in re.split(r\"(\\n)\", all_lines):\n",
    "        if line != \"\\n\":\n",
    "            string.append(line)\n",
    "\n",
    "df_clickbait = pd.DataFrame(string[0:len(string)-1],columns=[\"Text\"])\n",
    "df_clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Congress Slips CISA Into a Budget Bill That's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DUI Arrest Sparks Controversy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s unconstitutional to ban the homeless from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Government Error Just Revealed Snowden Was t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A toddler got meningitis. His anti-vac parents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>US releases Guantánamo prisoner after 14 years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>Loophole means ecstasy and loads of other drug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>Astronomers Watch a Supernova and See Reruns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>In Indian Rapists’ Neighborhood, Smoldering An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>Strong earthquake jolts Islamabad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1574 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text\n",
       "0     Congress Slips CISA Into a Budget Bill That's ...\n",
       "1                         DUI Arrest Sparks Controversy\n",
       "2     It’s unconstitutional to ban the homeless from...\n",
       "3     A Government Error Just Revealed Snowden Was t...\n",
       "4     A toddler got meningitis. His anti-vac parents...\n",
       "...                                                 ...\n",
       "1569  US releases Guantánamo prisoner after 14 years...\n",
       "1570  Loophole means ecstasy and loads of other drug...\n",
       "1571       Astronomers Watch a Supernova and See Reruns\n",
       "1572  In Indian Rapists’ Neighborhood, Smoldering An...\n",
       "1573                  Strong earthquake jolts Islamabad\n",
       "\n",
       "[1574 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading not-clickbait dataset.\n",
    "string = []    \n",
    "with open(\"not-clickbait\", \"r\") as f:\n",
    "    all_lines = f.read()\n",
    "    for line in re.split(r\"(\\n)\", all_lines):\n",
    "        if line != \"\\n\":\n",
    "            string.append(line)\n",
    "\n",
    "df_not_clickbait = pd.DataFrame(string[0:len(string)-1],columns=[\"Text\"])\n",
    "df_not_clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a label in the two datasets we read above.\n",
    "\n",
    "df_clickbait['label_code'] = 1\n",
    "df_not_clickbait['label_code'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the two datasets.\n",
    "data = pd.concat([df_clickbait, df_not_clickbait], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data to randomize.\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set Percentage: 72.0\n",
      "Validation set Percentage: 8.0\n",
      "Testing set Percentage: 20.0\n",
      "\n",
      "Training set distribution:\n",
      " 0    1141\n",
      "1     578\n",
      "Name: label_code, dtype: int64\n",
      "Validation set distribution:\n",
      " 0    123\n",
      "1     68\n",
      "Name: label_code, dtype: int64\n",
      "Testing set distribution:\n",
      " 0    310\n",
      "1    168\n",
      "Name: label_code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into train-validation-test datasets.\n",
    "train_text, test_text, train_label_code , test_label_code = train_test_split(data['Text'], data['label_code'], test_size = 0.20, random_state=1)\n",
    "train_text, validation_text, train_label_code, validation_label_code = train_test_split(train_text, train_label_code, test_size = 0.1, random_state=1)\n",
    "\n",
    "print('Training set Percentage:', round(100 * len(train_text)/len(data),0))\n",
    "print('Validation set Percentage:', round(100 * len(validation_text)/len(data),0))\n",
    "print('Testing set Percentage:', round(100 * len(test_text)/len(data),0))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print('Training set distribution:\\n', train_label_code.value_counts())\n",
    "print('Validation set distribution:\\n', validation_label_code.value_counts())\n",
    "print('Testing set distribution:\\n', test_label_code.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target rate of Training dataset: 34.0 %\n",
      "Target rate of Validation dataset: 36.0 %\n",
      "Target rate of Test dataset: 35.0 %\n"
     ]
    }
   ],
   "source": [
    "#Getting the value counts of each class to compute the target rate.\n",
    "train_not_clickbait, train_clickbait = train_label_code.value_counts()\n",
    "validation_not_clickbait, validation_clickbait = validation_label_code.value_counts()\n",
    "test_not_clickbait, test_clickbait = test_label_code.value_counts()\n",
    "\n",
    "#Computing the target rate of the datasets.\n",
    "train_target_rate = round(100 * train_clickbait / (train_clickbait + train_not_clickbait),0)\n",
    "validation_target_rate = round(100 * validation_clickbait / (validation_clickbait + validation_not_clickbait),0)\n",
    "test_target_rate = round(100 * test_clickbait / (test_clickbait + test_not_clickbait),0)\n",
    "\n",
    "#Printing the target rates of the datasets.\n",
    "print(\"Target rate of Training dataset:\", train_target_rate,\"%\")\n",
    "print(\"Target rate of Validation dataset:\", validation_target_rate,\"%\")\n",
    "print(\"Target rate of Test dataset:\", test_target_rate,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COMMENTS: The target rate looks pretty similar in all the datasets which is a good sign. All the datasets are imbalanced that is a different story. The not-clickbait samples are more than the clickbait samples in all the datasets which might be a bias problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming if we have a trivial baseline classifer that flags every text presented as clickbait, the precision, recall, and F1-score are as follows:\n",
    "<br><br>The target rate for my testing dataset is 35.0% which means the baseline classifier will classify these correctly.\n",
    "<br>So, True Positives = 0.35 or 35%\n",
    "<br>The remaining cases of the test dataset will also be classifed as Positive which in actual were Negative.\n",
    "<br>So, False Positives = 0.65 or 65%\n",
    "<br>\n",
    "<br>As the classifier does not classifies any text as Negative, the False Negatives = 0 and the True Negatives = 0.\n",
    "<br>\n",
    "<br>Precision is a measure of how many of the positive predictions made are correct (true positives). The formula for it is:\n",
    "<br>Precision = (True Positives) / (True Positives + False Positives) = (0.35) / (0.35 + 0.65) = 0.35\n",
    "<br><br>Recall is a measure of how many of the positive cases the classifier correctly predicted, over all the positive cases in the data. The formula for it is:\n",
    "<br>Recall  = (True Positives) / (True Positives + False Negatives) = (0.35) / (0.35 + 0) = 1.00\n",
    "<br><br>F1-Score is a measure combining both precision and recall. It is generally described as the harmonic mean of the two. Harmonic mean is just another way to calculate an “average” of values, generally described as more suitable for ratios (such as precision and recall) than the traditional arithmetic mean. The formula used for F1-score in this case is:\n",
    "<br>F1-Score = (2 * Precision * Recall) / (Precision + Recall) = (2 * 0.35 * 1) / (0.35 + 1) = (0.70) / (1.35) = 0.52\n",
    "<br><br>As per my understanding, F1-score depends on both Precision and Recall. It is really important that both these are balanced values. Consider an example where Precision = 0.01 and Recall = 1.00\n",
    "<br>F1-Score = (2 * 0.01 * 1) / (0.01 + 1) = 0.02\n",
    "<br>This is a very low F1-Score.\n",
    "<br>There might exists another good baseline classifier which has a Precision and Recall value of 1 and that gives us a F1-Score = (2 * 1 * 1) / (1 + 1) = 1 which is greater than 0.52 obtained previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier with Unigrams and Bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "1) Creating a pipeline to train the Naive Bayes classifier model.\n",
    "2) Fitting the training data using the classifier model.\n",
    "3) Get the predictions and compute the Precision, Recall, and F1-Score for training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the multinomial Naive Bayes classifier Pipeline:\n",
    "#Including the unigrams and bigrams.\n",
    "NB_pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer(ngram_range=(1, 2),stop_words = ENGLISH_STOP_WORDS)),\n",
    "    ('naive_bayes_classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vectorizer',\n",
       "                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                 stop_words=frozenset({'a', 'about', 'above',\n",
       "                                                       'across', 'after',\n",
       "                                                       'afterwards', 'again',\n",
       "                                                       'against', 'all',\n",
       "                                                       'almost', 'alone',\n",
       "                                                       'along', 'already',\n",
       "                                                       'also', 'although',\n",
       "                                                       'always', 'am', 'among',\n",
       "                                                       'amongst', 'amoungst',\n",
       "                                                       'amount', 'an', 'and',\n",
       "                                                       'another', 'any',\n",
       "                                                       'anyhow', 'anyone',\n",
       "                                                       'anything', 'anyway',\n",
       "                                                       'anywhere', ...}))),\n",
       "                ('naive_bayes_classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the training data.\n",
    "NB_pipeline.fit(train_text, train_label_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the predictions on training and validation datasets.\n",
    "train_predictions = NB_pipeline.predict(train_text)\n",
    "validation_predictions = NB_pipeline.predict(validation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Precision, Recall, and F1-Score for Training data.\n",
    "#Version 1.\n",
    "#Since our clickbait data is labelled as '1' & we want to choose this class as target class. We use pos_label = 1.\n",
    "train_precision = precision_score(train_label_code, train_predictions, pos_label=1)\n",
    "train_recall = recall_score(train_label_code, train_predictions, pos_label=1)\n",
    "train_f1_score = f1_score(train_label_code, train_predictions, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.99\n",
      "Training Recall: 1.0\n",
      "Training F1-Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "#Printing the training data's Precision, Recall, and F1-Score.\n",
    "print(\"Training Precision:\", round(train_precision,2))\n",
    "print(\"Training Recall:\", round(train_recall,2))\n",
    "print(\"Training F1-Score:\", round(train_f1_score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Precision, Recall, and F1-Score for Validation data.\n",
    "#Version 1.\n",
    "#Since our clickbait data is labelled as '1' & we want to choose this class as target class. We use pos_label = 1.\n",
    "validation_precision = precision_score(validation_label_code, validation_predictions, pos_label=1)\n",
    "validation_recall = recall_score(validation_label_code, validation_predictions, pos_label=1)\n",
    "validation_f1_score = f1_score(validation_label_code, validation_predictions, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.9\n",
      "Validation Recall: 0.69\n",
      "Validation F1-Score: 0.78\n"
     ]
    }
   ],
   "source": [
    "#Printing the validation data's Precision, Recall, and F1-Score.\n",
    "print(\"Validation Precision:\", round(validation_precision,2))\n",
    "print(\"Validation Recall:\", round(validation_recall,2))\n",
    "print(\"Validation F1-Score:\", round(validation_f1_score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Clickbait       1.00      1.00      1.00      1141\n",
      "    Clickbait       0.99      1.00      0.99       578\n",
      "\n",
      "     accuracy                           1.00      1719\n",
      "    macro avg       1.00      1.00      1.00      1719\n",
      " weighted avg       1.00      1.00      1.00      1719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Computing Precision, Recall, and F1-Score for Training data.\n",
    "#Version 2.\n",
    "print(classification_report(train_label_code, train_predictions, target_names=['Not Clickbait', 'Clickbait']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Clickbait       0.85      0.96      0.90       123\n",
      "    Clickbait       0.90      0.69      0.78        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.88      0.83      0.84       191\n",
      " weighted avg       0.87      0.86      0.86       191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Computing Precision, Recall, and F1-Score for Validation data.\n",
    "#Version 2.\n",
    "print(classification_report(validation_label_code, validation_predictions, target_names=['Not Clickbait', 'Clickbait']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COMMENTS: From the above classification reports, we can see that the classifier model performs very well on training data with the metrics as Precision = 0.99, Recall = 1.00, F1-Score = 0.99, Accuracy = 1.00 [target class: Clickbait]. This is almost the ideal performance.\n",
    "<br><br><b>Now, when we see the classifier's performance on validation data, it is also not bad. This can be verified from the metrics values as Precision = 0.90, Recall = 0.69, F1-Score = 0.78, Accuracy = 0.86 [target class: Clickbait]. Even though accuracy is high, we cannot just depend on it blindly, we need to consider the average recall and F1-Score. The imbalance in the dataset might be a reason for it because the metrics for Not Clickbait are better than Clickbait and we have more samples for Not Clickbait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "1) Tune at least 3 hyperparameters to try to improve the model's overall performance.\n",
    "2) Using the ParameterGrid class, performing a little grid search.\n",
    "3) Getting the best metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning the hyperparameters to improve the model's performance.\n",
    "parameters_grid = {\n",
    "    'count_vectorizer__max_df': [0.25, 0.5, 0.75, 1],\n",
    "    'naive_bayes_classifier__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "    'count_vectorizer__ngram_range': [(1, 1), (1, 2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'count_vectorizer__max_df': 0.25, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 0.01}\n",
      "Validation F1-Score: 0.77\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.85      0.93      0.89       123\n",
      "    Clickbait       0.84      0.71      0.77        68\n",
      "\n",
      "     accuracy                           0.85       191\n",
      "    macro avg       0.85      0.82      0.83       191\n",
      " weighted avg       0.85      0.85      0.84       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.25, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 0.1}\n",
      "Validation F1-Score: 0.79\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.88      0.91      0.89       123\n",
      "    Clickbait       0.83      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.85      0.84      0.84       191\n",
      " weighted avg       0.86      0.86      0.86       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.25, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 1.0}\n",
      "Validation F1-Score: 0.79\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.88      0.91      0.89       123\n",
      "    Clickbait       0.83      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.85      0.84      0.84       191\n",
      " weighted avg       0.86      0.86      0.86       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.25, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 10.0}\n",
      "Validation F1-Score: 0.58\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.75      0.99      0.86       123\n",
      "    Clickbait       0.97      0.41      0.58        68\n",
      "\n",
      "     accuracy                           0.79       191\n",
      "    macro avg       0.86      0.70      0.72       191\n",
      " weighted avg       0.83      0.79      0.76       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.25, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 0.01}\n",
      "Validation F1-Score: 0.76\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.85      0.90      0.88       123\n",
      "    Clickbait       0.80      0.72      0.76        68\n",
      "\n",
      "     accuracy                           0.84       191\n",
      "    macro avg       0.83      0.81      0.82       191\n",
      " weighted avg       0.84      0.84      0.84       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.25, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 0.1}\n",
      "Validation F1-Score: 0.79\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.87      0.90      0.89       123\n",
      "    Clickbait       0.81      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.85       191\n",
      "    macro avg       0.84      0.83      0.84       191\n",
      " weighted avg       0.85      0.85      0.85       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.25, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 1.0}\n",
      "Validation F1-Score: 0.78\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.85      0.96      0.90       123\n",
      "    Clickbait       0.90      0.69      0.78        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.88      0.83      0.84       191\n",
      " weighted avg       0.87      0.86      0.86       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.25, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 10.0}\n",
      "Validation F1-Score: 0.55\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.75      1.00      0.85       123\n",
      "    Clickbait       1.00      0.38      0.55        68\n",
      "\n",
      "     accuracy                           0.78       191\n",
      "    macro avg       0.87      0.69      0.70       191\n",
      " weighted avg       0.84      0.78      0.75       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.5, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 0.01}\n",
      "Validation F1-Score: 0.77\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.85      0.93      0.89       123\n",
      "    Clickbait       0.84      0.71      0.77        68\n",
      "\n",
      "     accuracy                           0.85       191\n",
      "    macro avg       0.85      0.82      0.83       191\n",
      " weighted avg       0.85      0.85      0.84       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.5, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 0.1}\n",
      "Validation F1-Score: 0.79\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.88      0.91      0.89       123\n",
      "    Clickbait       0.83      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.85      0.84      0.84       191\n",
      " weighted avg       0.86      0.86      0.86       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.5, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 1.0}\n",
      "Validation F1-Score: 0.79\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.88      0.91      0.89       123\n",
      "    Clickbait       0.83      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.85      0.84      0.84       191\n",
      " weighted avg       0.86      0.86      0.86       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.5, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 10.0}\n",
      "Validation F1-Score: 0.58\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.75      0.99      0.86       123\n",
      "    Clickbait       0.97      0.41      0.58        68\n",
      "\n",
      "     accuracy                           0.79       191\n",
      "    macro avg       0.86      0.70      0.72       191\n",
      " weighted avg       0.83      0.79      0.76       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.5, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 0.01}\n",
      "Validation F1-Score: 0.76\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.85      0.90      0.88       123\n",
      "    Clickbait       0.80      0.72      0.76        68\n",
      "\n",
      "     accuracy                           0.84       191\n",
      "    macro avg       0.83      0.81      0.82       191\n",
      " weighted avg       0.84      0.84      0.84       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.5, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 0.1}\n",
      "Validation F1-Score: 0.79\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.87      0.90      0.89       123\n",
      "    Clickbait       0.81      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.85       191\n",
      "    macro avg       0.84      0.83      0.84       191\n",
      " weighted avg       0.85      0.85      0.85       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.5, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 1.0}\n",
      "Validation F1-Score: 0.78\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.85      0.96      0.90       123\n",
      "    Clickbait       0.90      0.69      0.78        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.88      0.83      0.84       191\n",
      " weighted avg       0.87      0.86      0.86       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.5, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 10.0}\n",
      "Validation F1-Score: 0.55\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.75      1.00      0.85       123\n",
      "    Clickbait       1.00      0.38      0.55        68\n",
      "\n",
      "     accuracy                           0.78       191\n",
      "    macro avg       0.87      0.69      0.70       191\n",
      " weighted avg       0.84      0.78      0.75       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.75, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 0.01}\n",
      "Validation F1-Score: 0.77\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.85      0.93      0.89       123\n",
      "    Clickbait       0.84      0.71      0.77        68\n",
      "\n",
      "     accuracy                           0.85       191\n",
      "    macro avg       0.85      0.82      0.83       191\n",
      " weighted avg       0.85      0.85      0.84       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.75, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 0.1}\n",
      "Validation F1-Score: 0.79\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.88      0.91      0.89       123\n",
      "    Clickbait       0.83      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.85      0.84      0.84       191\n",
      " weighted avg       0.86      0.86      0.86       191\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'count_vectorizer__max_df': 0.75, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 1.0}\n",
      "Validation F1-Score: 0.79\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.88      0.91      0.89       123\n",
      "    Clickbait       0.83      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.85      0.84      0.84       191\n",
      " weighted avg       0.86      0.86      0.86       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.75, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 10.0}\n",
      "Validation F1-Score: 0.58\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.75      0.99      0.86       123\n",
      "    Clickbait       0.97      0.41      0.58        68\n",
      "\n",
      "     accuracy                           0.79       191\n",
      "    macro avg       0.86      0.70      0.72       191\n",
      " weighted avg       0.83      0.79      0.76       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.75, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 0.01}\n",
      "Validation F1-Score: 0.76\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.85      0.90      0.88       123\n",
      "    Clickbait       0.80      0.72      0.76        68\n",
      "\n",
      "     accuracy                           0.84       191\n",
      "    macro avg       0.83      0.81      0.82       191\n",
      " weighted avg       0.84      0.84      0.84       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.75, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 0.1}\n",
      "Validation F1-Score: 0.79\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.87      0.90      0.89       123\n",
      "    Clickbait       0.81      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.85       191\n",
      "    macro avg       0.84      0.83      0.84       191\n",
      " weighted avg       0.85      0.85      0.85       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.75, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 1.0}\n",
      "Validation F1-Score: 0.78\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.85      0.96      0.90       123\n",
      "    Clickbait       0.90      0.69      0.78        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.88      0.83      0.84       191\n",
      " weighted avg       0.87      0.86      0.86       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 0.75, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 10.0}\n",
      "Validation F1-Score: 0.55\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.75      1.00      0.85       123\n",
      "    Clickbait       1.00      0.38      0.55        68\n",
      "\n",
      "     accuracy                           0.78       191\n",
      "    macro avg       0.87      0.69      0.70       191\n",
      " weighted avg       0.84      0.78      0.75       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 1, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 0.01}\n",
      "Validation F1-Score: 0.31\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.66      0.81      0.73       123\n",
      "    Clickbait       0.42      0.25      0.31        68\n",
      "\n",
      "     accuracy                           0.61       191\n",
      "    macro avg       0.54      0.53      0.52       191\n",
      " weighted avg       0.58      0.61      0.58       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 1, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 0.1}\n",
      "Validation F1-Score: 0.31\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.66      0.81      0.73       123\n",
      "    Clickbait       0.42      0.25      0.31        68\n",
      "\n",
      "     accuracy                           0.61       191\n",
      "    macro avg       0.54      0.53      0.52       191\n",
      " weighted avg       0.58      0.61      0.58       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 1, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 1.0}\n",
      "Validation F1-Score: 0.29\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.66      0.85      0.74       123\n",
      "    Clickbait       0.44      0.22      0.29        68\n",
      "\n",
      "     accuracy                           0.62       191\n",
      "    macro avg       0.55      0.53      0.52       191\n",
      " weighted avg       0.58      0.62      0.58       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 1, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 10.0}\n",
      "Validation F1-Score: 0.0\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.64      1.00      0.78       123\n",
      "    Clickbait       0.00      0.00      0.00        68\n",
      "\n",
      "     accuracy                           0.64       191\n",
      "    macro avg       0.32      0.50      0.39       191\n",
      " weighted avg       0.41      0.64      0.50       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 1, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 0.01}\n",
      "Validation F1-Score: 0.4\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.69      0.81      0.75       123\n",
      "    Clickbait       0.50      0.34      0.40        68\n",
      "\n",
      "     accuracy                           0.64       191\n",
      "    macro avg       0.59      0.58      0.57       191\n",
      " weighted avg       0.62      0.64      0.62       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 1, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 0.1}\n",
      "Validation F1-Score: 0.4\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.69      0.80      0.74       123\n",
      "    Clickbait       0.49      0.34      0.40        68\n",
      "\n",
      "     accuracy                           0.64       191\n",
      "    macro avg       0.59      0.57      0.57       191\n",
      " weighted avg       0.62      0.64      0.62       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 1, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 1.0}\n",
      "Validation F1-Score: 0.39\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.69      0.88      0.77       123\n",
      "    Clickbait       0.57      0.29      0.39        68\n",
      "\n",
      "     accuracy                           0.67       191\n",
      "    macro avg       0.63      0.59      0.58       191\n",
      " weighted avg       0.65      0.67      0.64       191\n",
      "\n",
      "\n",
      "Parameters: {'count_vectorizer__max_df': 1, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes_classifier__alpha': 10.0}\n",
      "Validation F1-Score: 0.0\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.64      1.00      0.78       123\n",
      "    Clickbait       0.00      0.00      0.00        68\n",
      "\n",
      "     accuracy                           0.64       191\n",
      "    macro avg       0.32      0.50      0.39       191\n",
      " weighted avg       0.41      0.64      0.50       191\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the ParameterGrid class,\n",
    "#We perform a grid search to check if the model performs better at any other values of the hyperparameters.\n",
    "\n",
    "#Initializing the best metrics as 0 or None.\n",
    "best_f1_score = 0\n",
    "best_parameters = None\n",
    "best_report = None\n",
    "\n",
    "#We repeat the fit and predict processes on different values of inputs as a grid search to get the best results.\n",
    "for parameters in ParameterGrid(parameters_grid):\n",
    "    NB_pipeline.set_params(**parameters)\n",
    "\n",
    "    NB_pipeline.fit(train_text, train_label_code)\n",
    "\n",
    "    validation_predictions = NB_pipeline.predict(validation_text)\n",
    "\n",
    "    overall_report = classification_report(validation_label_code, validation_predictions, target_names=[\"Non-Clickbait\", \"Clickbait\"], zero_division = 0)\n",
    "    validation_f1_score = f1_score(validation_label_code, validation_predictions, pos_label=1)\n",
    "\n",
    "    print(\"Parameters:\", parameters)\n",
    "    print(\"Validation F1-Score:\", round(validation_f1_score,2))\n",
    "    print(overall_report)\n",
    "    print()\n",
    "\n",
    "    if validation_f1_score > best_f1_score:\n",
    "        best_f1_score = validation_f1_score\n",
    "        best_parameters = parameters\n",
    "        best_report = overall_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'count_vectorizer__max_df': 0.25, 'count_vectorizer__ngram_range': (1, 1), 'naive_bayes_classifier__alpha': 0.1}\n",
      "\n",
      "Best Validation F1-Score: 0.79\n",
      "\n",
      "Best Validation Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.88      0.91      0.89       123\n",
      "    Clickbait       0.83      0.76      0.79        68\n",
      "\n",
      "     accuracy                           0.86       191\n",
      "    macro avg       0.85      0.84      0.84       191\n",
      " weighted avg       0.86      0.86      0.86       191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Getting the best metrics.\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"\\nBest Validation F1-Score:\", round(best_f1_score,2))\n",
    "print(\"\\nBest Validation Metrics:\")\n",
    "print(best_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COMMENTS: Before Hyperparameter tuning, the validation dataset metrics were as follows:\n",
    "<br><b>Precision = 0.90, Recall = 0.69, F1-Score = 0.78, Accuracy = 0.86 [target class: Clickbait]\n",
    "<br><br><b>After Hyperparameter tuning, the validation dataset metrics that we found for the best model using parameter grid are as follows:\n",
    "<br><b>Precision = 0.83, Recall = 0.76, F1-Score = 0.79, Accuracy = 0.86 [target class: Clickbait]\n",
    "<br><br><b>This clearly shows that the hyperparameter tuning did an improvement in Recall and F1-score. A more improvement was expected. However, any improvement is an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "1) Using the best model parameters, we will select this as our model and fit our training data.\n",
    "2) Make predictions on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vectorizer',\n",
       "                 CountVectorizer(max_df=0.25,\n",
       "                                 stop_words=frozenset({'a', 'about', 'above',\n",
       "                                                       'across', 'after',\n",
       "                                                       'afterwards', 'again',\n",
       "                                                       'against', 'all',\n",
       "                                                       'almost', 'alone',\n",
       "                                                       'along', 'already',\n",
       "                                                       'also', 'although',\n",
       "                                                       'always', 'am', 'among',\n",
       "                                                       'amongst', 'amoungst',\n",
       "                                                       'amount', 'an', 'and',\n",
       "                                                       'another', 'any',\n",
       "                                                       'anyhow', 'anyone',\n",
       "                                                       'anything', 'anyway',\n",
       "                                                       'anywhere', ...}))),\n",
       "                ('naive_bayes_classifier', MultinomialNB(alpha=0.1))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the best parameters as our parameters in model pipeline.\n",
    "NB_pipeline.set_params(**best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vectorizer',\n",
       "                 CountVectorizer(max_df=0.25,\n",
       "                                 stop_words=frozenset({'a', 'about', 'above',\n",
       "                                                       'across', 'after',\n",
       "                                                       'afterwards', 'again',\n",
       "                                                       'against', 'all',\n",
       "                                                       'almost', 'alone',\n",
       "                                                       'along', 'already',\n",
       "                                                       'also', 'although',\n",
       "                                                       'always', 'am', 'among',\n",
       "                                                       'amongst', 'amoungst',\n",
       "                                                       'amount', 'an', 'and',\n",
       "                                                       'another', 'any',\n",
       "                                                       'anyhow', 'anyone',\n",
       "                                                       'anything', 'anyway',\n",
       "                                                       'anywhere', ...}))),\n",
       "                ('naive_bayes_classifier', MultinomialNB(alpha=0.1))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the classifier model on the training dataset with the best parameters.\n",
    "NB_pipeline.fit(train_text, train_label_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Predictions on the test dataset.\n",
    "test_predictions = NB_pipeline.predict(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Precision, Recall, and F1-Score for Testing data.\n",
    "#Version 1.\n",
    "#Since our clickbait data is labelled as '1' & we want to choose this class as target class. We use pos_label = 1.\n",
    "test_precision = precision_score(test_label_code, test_predictions, pos_label=1)\n",
    "test_recall = recall_score(test_label_code, test_predictions, pos_label=1)\n",
    "test_f1_score = f1_score(test_label_code, test_predictions, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Precision: 0.84\n",
      "Testing Recall: 0.78\n",
      "Testing F1-Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "#Printing the test data's Precision, Recall, and F1-Score.\n",
    "print(\"Testing Precision:\", round(test_precision,2))\n",
    "print(\"Testing Recall:\", round(test_recall,2))\n",
    "print(\"Testing F1-Score:\", round(test_f1_score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Clickbait       0.89      0.92      0.90       310\n",
      "    Clickbait       0.84      0.78      0.81       168\n",
      "\n",
      "     accuracy                           0.87       478\n",
      "    macro avg       0.86      0.85      0.86       478\n",
      " weighted avg       0.87      0.87      0.87       478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Computing Precision, Recall, and F1-Score for Testing data.\n",
    "#Version 2.\n",
    "print(classification_report(test_label_code, test_predictions, target_names=['Not Clickbait', 'Clickbait']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COMMENTS: On evaluating this model's performance on Test data, we can see that the model performs better on the test data as compared to the validation data. As long as the metrics of precision and recall are high and almost equal, they indicate a good model.\n",
    "<br><br><b>The model's metrics on test data are as follows:\n",
    "<br><b>Precision = 0.84, Recall = 0.78, F1-Score = 0.81, Accuracy = 0.87 [target class: Clickbait]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "1) Now, we want to find the words which our machine identified as the key indicators of our target class \"clickbait\".\n",
    "2) Using the log probabilities, find the top 5 key indicators of clickbait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the list of words/features from our vectorizer in the pipeline.\n",
    "feature_names = NB_pipeline.named_steps['count_vectorizer'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the log probabilities for the indicators of \"clickbait\" (target class)\n",
    "#from the naive bayes classifier in the pipeline.\n",
    "log_probabilities = NB_pipeline.named_steps['naive_bayes_classifier'].feature_log_prob_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forming a key-value pair of words/features and their log probabilities to get the top 5.\n",
    "features_log_probabilities = dict(zip(feature_names, log_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the dictionary in descending order based on the log probabilities of indicators.\n",
    "sorted_features = sorted(features_log_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Selecting the top 5 words as strong clickbait indicators.\n",
    "top_clickbait_indicators = [indicator for indicator, log_probability in sorted_features[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 5 Key Indicators Of Clickbait Are:\n",
      "1. believe\n",
      "2. won\n",
      "3. ll\n",
      "4. new\n",
      "5. guess\n"
     ]
    }
   ],
   "source": [
    "#Printing the top 5 clickbait indicators we identified from the dictionary.\n",
    "print(\"The Top 5 Key Indicators Of Clickbait Are:\")\n",
    "for s_no, indicator in enumerate(top_clickbait_indicators, 1):\n",
    "    print(f\"{s_no}. {indicator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COMMENTS: A general speculation of the data gave a little bit idea about why these words might have been identified by the model as the key indicators.\n",
    "<br><br><b>believe: This word was used as a clickbait to trap a lot of people by providing them impossible/unBELIVable deals, hence the use of word 'believe' was used a lot as \"won't believe\" which means unbelivable.\n",
    "<br><br><b>won: This word was used as a clickbait to trap a lot of people by either telling them that they have WON something or providing them impossible/unbelivable deals, hence the use of word \"won't\" was used a lot as \"won't believe\" which means unbelivable.\n",
    "<br><br><b>ll: This word was not used directly as a clickbait but it was a part of a contaction which we often use in English like We'll, you'll. Since this kind of text was present a lot of times in the data where the scammer addressed the victim say as \"You'll never believe\" which caused this word/token to show up with top 5 log probabilities.\n",
    "<br><br><b>new: This word was used as a clickbait to trap a lot of people by either telling them that they have won something new, hence the use of word \"new\" was used a lot as \"won new PS5\", etc.\n",
    "<br><br><b>guess: This word was used as a clickbait to trap a lot of people by providing them impossible/unbelivable deals, hence the use of word \"guess\" was used a lot as \"never guess\" which means something shocking which tempts the victim to fall in the trap of clickbait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "1) This is a fun and interesting part. Rather than using the classifier, we will use a regex in this problem and in that regex we will use the top 5 indicators of the target class as our hits for clickbait.\n",
    "2) If anyone of the key indicator is present in our text sentence, we mark it as our target class else not the target class.\n",
    "3) Compute the metrics for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a regular expression pattern that allows to search for a regular expression match object,\n",
    "#with any of the top 5 indicators we found in last step.\n",
    "pattern = r'\\b(?:' + '|'.join(re.escape(indicator) for indicator in top_clickbait_indicators) + r')\\b'\n",
    "\n",
    "#Function to search if top keyword exists in the text using the regex pattern (IGNORING THE CASE).\n",
    "def top_indicator_detector(text):\n",
    "    return re.search(pattern, text, flags=re.IGNORECASE) is not None\n",
    "\n",
    "#Applying the function on the test set.\n",
    "test_predictions = (test_text.apply(top_indicator_detector)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Precision, Recall, and F1-Score for Testing data.\n",
    "#Version 1.\n",
    "#Since our clickbait data is labelled as '1' & we want to choose this class as target class. We use pos_label = 1.\n",
    "test_precision = precision_score(test_label_code, test_predictions, pos_label=1)\n",
    "test_recall = recall_score(test_label_code, test_predictions, pos_label=1)\n",
    "test_f1_score = f1_score(test_label_code, test_predictions, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Precision: 0.63\n",
      "Testing Recall: 0.16\n",
      "Testing F1-Score: 0.26\n"
     ]
    }
   ],
   "source": [
    "#Printing the test data's Precision, Recall, and F1-Score.\n",
    "print(\"Testing Precision:\", round(test_precision,2))\n",
    "print(\"Testing Recall:\", round(test_recall,2))\n",
    "print(\"Testing F1-Score:\", round(test_f1_score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Clickbait       0.68      0.95      0.79       310\n",
      "    Clickbait       0.63      0.16      0.26       168\n",
      "\n",
      "     accuracy                           0.67       478\n",
      "    macro avg       0.65      0.55      0.52       478\n",
      " weighted avg       0.66      0.67      0.60       478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Computing Precision, Recall, and F1-Score for Testing data.\n",
    "#Version 2.\n",
    "print(classification_report(test_label_code, test_predictions, target_names=['Not Clickbait', 'Clickbait']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COMMENTS: When we tried this fun experiment which uses regex to identify the text as clickbait or not, we can see that it performs as a low to average classifier. It is better than a random classifier which might have an accuracy of 50%. However, the recall rate and F1-Score is very low. It is expected as we are just using top 5 predictors. Also, the cases with contractions created problem, the cases with New Jersey, New York created problem due the indicator word \"new\". There might be other problems as well which might not be visible directly but could be found when delved deeper into this.\n",
    "<br><br><b>The metrics we get for this are as follows:\n",
    "<br><b>Precision = 0.63, Recall = 0.16, F1-Score = 0.26, Accuracy = 0.67 [target class Clickbait]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compairing Results obtained so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare the rules based classifier with our machine learning solution, we can clearly see from the metrics that our machine-learning solution perform the best.\n",
    "<br>Rules Based: Precision = 0.63, Recall = 0.16, F1-Score = 0.26, Accuracy = 0.67\n",
    "<br>Machine-learning Based: Precision = 0.84, Recall = 0.78, F1-Score = 0.81, Accuracy = 0.87\n",
    "<br><br>The reason it perform better is that machine learning solution uses the count vectorizer and considers each word into its training and improving. The rules based model just uses top 5 words to identify the target class. The performance is not that bad when we think that with just 5 words, we got a precision of 0.63 and accuracy of 0.67. But the machine learning classifier performs way better than the rules based solution.\n",
    "<br>When we compare the baseline classifier with the other two models, the baseline classifier and the rules based classifier are both low grade classifiers as one gives high precision and the other gives high recall, whereas we are finding the sweetspot where the precision and recall are high and almost equal.\n",
    "<br><br>Rules Based: Precision = 0.63, Recall = 0.16, F1-Score = 0.26, Accuracy = 0.67\n",
    "<br>Machine-learning Based: Precision = 0.84, Recall = 0.78, F1-Score = 0.81, Accuracy = 0.87\n",
    "<br>Baseline classifier: Precision = 0.35, Recall = 1, F1-Score = 0.51, Accuracy = 0.35"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
